{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81d424f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: pyodbc 4.0.0-unsupported has a non-standard version number. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pyodbc or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q sql av pillow pandas sqlalchemy ipython-sql pymysql roboflow git+https://github.com/THU-MIG/yolov10.git supervision huggingface_hub bottleneck==1.3.6 numexpr==2.8.4 wget gcloud google google-cloud-storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e529f552",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: httplib2==0.15.0 in /mnt/linuxlab/home/mstaus1/anaconda3/lib/python3.9/site-packages (0.15.0)\n",
      "Requirement already satisfied: google-api-python-client==1.6 in /mnt/linuxlab/home/mstaus1/anaconda3/lib/python3.9/site-packages (1.6.0)\n",
      "Requirement already satisfied: oauth2client<5.0.0dev,>=1.5.0 in /mnt/linuxlab/home/mstaus1/anaconda3/lib/python3.9/site-packages (from google-api-python-client==1.6) (4.1.3)\n",
      "Requirement already satisfied: six<2dev,>=1.6.1 in /mnt/linuxlab/home/mstaus1/anaconda3/lib/python3.9/site-packages (from google-api-python-client==1.6) (1.16.0)\n",
      "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /mnt/linuxlab/home/mstaus1/anaconda3/lib/python3.9/site-packages (from google-api-python-client==1.6) (3.0.1)\n",
      "Requirement already satisfied: pyasn1>=0.1.7 in /mnt/linuxlab/home/mstaus1/anaconda3/lib/python3.9/site-packages (from oauth2client<5.0.0dev,>=1.5.0->google-api-python-client==1.6) (0.6.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.0.5 in /mnt/linuxlab/home/mstaus1/anaconda3/lib/python3.9/site-packages (from oauth2client<5.0.0dev,>=1.5.0->google-api-python-client==1.6) (0.4.0)\n",
      "Requirement already satisfied: rsa>=3.1.4 in /mnt/linuxlab/home/mstaus1/anaconda3/lib/python3.9/site-packages (from oauth2client<5.0.0dev,>=1.5.0->google-api-python-client==1.6) (4.0)\n",
      "\u001b[33mDEPRECATION: pyodbc 4.0.0-unsupported has a non-standard version number. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pyodbc or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install httplib2==0.15.0 google-api-python-client==1.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3e0d6be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gcloud in c:\\python38\\lib\\site-packages (0.18.3)\n",
      "Requirement already satisfied: httplib2>=0.9.1 in c:\\python38\\lib\\site-packages (from gcloud) (0.22.0)\n",
      "Requirement already satisfied: googleapis-common-protos in c:\\python38\\lib\\site-packages (from gcloud) (1.63.2)\n",
      "Requirement already satisfied: oauth2client>=2.0.1 in c:\\python38\\lib\\site-packages (from gcloud) (4.1.3)\n",
      "Requirement already satisfied: protobuf!=3.0.0.b2.post1,>=3.0.0b2 in c:\\python38\\lib\\site-packages (from gcloud) (5.27.2)\n",
      "Requirement already satisfied: six in c:\\python38\\lib\\site-packages (from gcloud) (1.16.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in c:\\python38\\lib\\site-packages (from httplib2>=0.9.1->gcloud) (3.1.2)\n",
      "Requirement already satisfied: pyasn1>=0.1.7 in c:\\python38\\lib\\site-packages (from oauth2client>=2.0.1->gcloud) (0.6.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.0.5 in c:\\python38\\lib\\site-packages (from oauth2client>=2.0.1->gcloud) (0.4.0)\n",
      "Requirement already satisfied: rsa>=3.1.4 in c:\\python38\\lib\\site-packages (from oauth2client>=2.0.1->gcloud) (4.9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install gcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ad591b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: oauth2client in c:\\python38\\lib\\site-packages (4.1.3)\n",
      "Requirement already satisfied: httplib2>=0.9.1 in c:\\python38\\lib\\site-packages (from oauth2client) (0.22.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.7 in c:\\python38\\lib\\site-packages (from oauth2client) (0.6.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.0.5 in c:\\python38\\lib\\site-packages (from oauth2client) (0.4.0)\n",
      "Requirement already satisfied: rsa>=3.1.4 in c:\\python38\\lib\\site-packages (from oauth2client) (4.9)\n",
      "Requirement already satisfied: six>=1.6.1 in c:\\python38\\lib\\site-packages (from oauth2client) (1.16.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in c:\\python38\\lib\\site-packages (from httplib2>=0.9.1->oauth2client) (3.1.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install oauth2client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07611921",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install protobuf==3.20.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9de8481",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pymysql\n",
    "from pymysql.err import IntegrityError, OperationalError\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import supervision as sv\n",
    "import cv2\n",
    "import av\n",
    "from PIL import Image\n",
    "import shutil as sh\n",
    "import re\n",
    "from roboflow import Roboflow\n",
    "import cv2\n",
    "from ultralytics import YOLOv10\n",
    "import wget\n",
    "from time import time\n",
    "from gcloud import storage\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d63ffedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_folder = os.path.join('.', 'Files_local')\n",
    "temp_weights = os.path.join(temp_folder, 'Weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f33741e",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_main = 'test_4'\n",
    "cur_name = 'ab'\n",
    "\n",
    "sql_conn = pymysql.connect(\n",
    "    user=\"root\",\n",
    "    password=\"dbuserdbuser\",\n",
    "    host=\"localhost\",\n",
    "    port=3309,\n",
    "    database=db_main,\n",
    "    cursorclass=pymysql.cursors.DictCursor,\n",
    "    autocommit=True)\n",
    "cur = sql_conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8310e24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bba = sv.BoxCornerAnnotator()\n",
    "la = sv.LabelAnnotator(text_scale = 0.4, text_padding = 1)\n",
    "\n",
    "def annotate_image(input_img, model, label_annotator = la, bounding_box_annotator = bba, verbose = False, conf = True, conf_level = 0.05):\n",
    "    \n",
    "    cont_img = np.ascontiguousarray(input_img, dtype=np.uint8)\n",
    "    results = model(cont_img, conf=conf_level, verbose = verbose)[0]\n",
    "    conf_array = np.array(results.boxes.conf.cpu())\n",
    "    conf_ls_str  = [str(round(x * 100) ) + '%' for x in conf_array]\n",
    "    tot_time = sum([x for x in results.speed.values()])\n",
    "    \n",
    "    detections = sv.Detections.from_ultralytics(results)\n",
    "    annotated_image = bounding_box_annotator.annotate(\n",
    "        scene=input_img, detections=detections)\n",
    "    num_oysters = detections.xyxy.shape[0]\n",
    "    annotated_image = label_annotator.annotate(\n",
    "        scene=annotated_image, detections=detections, labels = conf_ls_str)\n",
    "    return(annotated_image, num_oysters, tot_time, detections)\n",
    "\n",
    "\n",
    "def annotate_video(input_vid, model, out_location = '.', im_width = 416, im_height = 416, conf_level = 0.3):\n",
    "    tot_oysters = 0\n",
    "    tot_frame = 0\n",
    "    \n",
    "    bba = sv.BoundingBoxAnnotator()\n",
    "    la = sv.LabelAnnotator()\n",
    "\n",
    "    container = av.open(input_vid)\n",
    "    stream_vid = container.streams.video[0]\n",
    "    fname = input_vid.rsplit('/', 1)[-1]\n",
    "    per_index = fname.index('.')\n",
    "    out_path = os.path.join(out_location, f'{fname[:per_index]}_annotated.mp4')\n",
    "    outp = av.open(out_path, 'w')\n",
    "    codec_name = stream_vid.codec_context.name\n",
    "    fps = stream_vid.codec_context.rate\n",
    "    output_stream = outp.add_stream(codec_name, str(fps))\n",
    "    output_stream.width = im_width\n",
    "    output_stream.height = im_height\n",
    "    output_stream.pix_fmt = stream_vid.codec_context.pix_fmt\n",
    "    start = time()\n",
    "    for index, frame in enumerate(container.decode(stream_vid)):\n",
    "        pil_img = frame.to_image()\n",
    "        np_img = np.array(pil_img)\n",
    "        np_img_resize = cv2.resize(np_img, (im_width, im_height))\n",
    "        np_rot = np_img_resize[:, :, ::-1]\n",
    "        small_pil_img = Image.fromarray(np_rot)\n",
    "        np_image_2 = np.array(small_pil_img)\n",
    "        an_mg, num_oysters, _, _2z = annotate_image(np_image_2, model, conf_level = conf_level)\n",
    "        tot_oysters += num_oysters\n",
    "        frame_out = av.VideoFrame.from_ndarray(an_mg, format='bgr24')\n",
    "        pkt = output_stream.encode(frame_out)\n",
    "        outp.mux(pkt)\n",
    "    end = time()\n",
    "    net_time = end - start\n",
    "    container.close()\n",
    "    outp.close()\n",
    "    ann_rate = (index / fps) / net_time # ratio of time to annotate versus length of video\n",
    "    return tot_oysters / index, net_time, out_path, ann_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9327bfd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ext(fpath):\n",
    "    fname = fpath.rsplit('/', 1)[-1]\n",
    "    per_index = fname.index('.')\n",
    "    ext = fname[per_index + 1:].lower()\n",
    "    return ext\n",
    "\n",
    "def get_id_fname(f_out, fpath, id):\n",
    "    fname = fpath.rsplit('/', 1)[-1]\n",
    "    per_index = fname.index('.')\n",
    "    ext = fname[per_index + 1:].lower()\n",
    "    return os.path.join(f_out, f\"{fname[:per_index]}-{id}.{ext}\")\n",
    "\n",
    "def get_REPLACE_ID(column_id = 'Raw_File_ID', table='raw_files', column_rep='Filepath'):\n",
    "    cur.execute(f\"\"\"SELECT {column_id} from {table} where {column_rep} = 'REPLACE'\"\"\")\n",
    "    id = cur.fetchall()[-1][column_id]\n",
    "    return id\n",
    "\n",
    "def get_match(pattern, string):\n",
    "    matches = re.search(pattern, string)\n",
    "    if matches:\n",
    "        print(matches.group(1))\n",
    "        return(matches.group(1))\n",
    "    else:\n",
    "        raise ValueError(f'Improper string exported, couldn\\'t find {pattern} in the given text')\n",
    "\n",
    "\n",
    "\n",
    "def delete_folder(name, base = \".\"):\n",
    "    fpath = os.path.join(base, name)\n",
    "    if os.path.exists(fpath):\n",
    "        for item in os.listdir(fpath):\n",
    "            new_path = os.path.join(fpath, item)\n",
    "            if os.path.isfile(new_path):\n",
    "                os.remove(new_path)\n",
    "            elif os.path.isdir(new_path):\n",
    "                delete_folder(item, fpath)\n",
    "        os.rmdir(fpath)\n",
    "                \n",
    "def make_folder(fpath, overwrite = False):\n",
    "    delete_folder(fpath, '.') if overwrite else None\n",
    "    if os.path.exists(fpath):\n",
    "        \n",
    "        print(f\"{fpath} already exists\")\n",
    "    else:\n",
    "        os.mkdir(fpath)\n",
    "        \n",
    "def get_temp_fname(fpath, temp_f = temp_folder):\n",
    "    fname = fpath.rsplit('/', 1)[-1]\n",
    "    return os.path.join(temp_folder, fname)\n",
    "\n",
    "def trim_file_depth(fpath, start = 0, end = 'Unused'):\n",
    "    split_ls = fpath.rsplit('/')\n",
    "    if end == 'Unused':\n",
    "        end = len(split_ls)\n",
    "    return '/'.join(split_ls[start:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf641330",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def sign_in_storage_g(JSON_file = 'molten-album-427115-q6-cfa4e4aaf3bd.json'):\n",
    "    f = open(JSON_file)\n",
    "    credentials_dict = json.load(f)\n",
    "    credentials = ServiceAccountCredentials.from_json_keyfile_dict(credentials_dict)\n",
    "    client = storage.Client(credentials=credentials, project='molten-album-427115-q6')\n",
    "    bkt = client.get_bucket('test_bucket_abc123')\n",
    "    return bkt, client\n",
    "\n",
    "b, cl = sign_in_storage_g()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e07771b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def check_folder_exists_g(folder_to_upload, check_location, disable = False, full = False, bkt = b): # assume full installation\n",
    "    # assumes if one file is in the right location then they all are\n",
    "    try:\n",
    "        if disable:\n",
    "            return True\n",
    "        cur_path = folder_to_upload\n",
    "        path_check_exist_g = check_location\n",
    "        if not os.path.exists(cur_path):\n",
    "            return False\n",
    "        all_items = os.listdir(cur_path)\n",
    "        for added_item in all_items: # searching for a file\n",
    "            cur_path = os.path.join(cur_path, added_item)\n",
    "            path_check_exist_g = os.path.join(path_check_exist_g, added_item)\n",
    "            cur_exists = False\n",
    "            if os.path.isfile(cur_path): # if you just want to make sure one file is the same, set full to \n",
    "                cur_exists = path_exists_g(path_check_exist_g)\n",
    "                if not full: \n",
    "                    return cur_exists\n",
    "            elif os.path.isdir(cur_path):\n",
    "                cur_exists = check_folder_exists_g(cur_path, path_check_exist_g)\n",
    "            if not cur_exists:\n",
    "                return False\n",
    "        return True\n",
    "    except Exception:\n",
    "        print(f\"Current folder {folder_to_upload} or {check_location} is malformed\")\n",
    "        return False\n",
    "\n",
    "def upload_file_g(file_to_upload, new_name_with_path_g, bkt = b):\n",
    "    blob = bkt.blob(new_name_with_path_g)\n",
    "    blob.upload_from_filename(file_to_upload)\n",
    "    \n",
    "\n",
    "def upload_folder_g(folder_to_upload, new_name_with_path_g, bkt = b , level = 0, overwrite = False):\n",
    "    already_uploaded = False\n",
    "    if level == 0: # check if already uploaded\n",
    "        already_uploaded = check_folder_exists_g(folder_to_upload, new_name_with_path_g)\n",
    "        level += 1\n",
    "    if os.path.exists(folder_to_upload) and (not already_uploaded or overwrite):\n",
    "        for item in os.listdir(folder_to_upload):\n",
    "            cur_path = os.path.join(folder_to_upload, item)\n",
    "            new_path_g = os.path.join(new_name_with_path_g, item)\n",
    "            if os.path.isfile(cur_path):\n",
    "                upload_file_g(cur_path, new_path_g)\n",
    "            elif os.path.isdir(new_path_g):\n",
    "                upload_folder_g(cur_path, new_path_g, level)\n",
    "\n",
    "def download_file_g(file_to_download_g, new_name_with_path, bkt = b):\n",
    "    if not os.path.exists(new_name_with_path):\n",
    "        blob = bkt.blob(file_to_download_g)\n",
    "        blob.download_to_filename(new_name_with_path)\n",
    "\n",
    "def list_blobs_g(folder_base, bkt = b, client = cl):\n",
    "    \"\"\"Lists all the blobs in the bucket that begin with the prefix.\"\"\"\n",
    "    blobs = bkt.list_blobs(prefix=folder_base)\n",
    "    return blobs\n",
    "\n",
    "def download_folder_g(folder_to_download_g, new_name_with_path, overwrite = False, level = 0, file_depth = 3):\n",
    "    already_uploaded = False\n",
    "    if level == 0: # check if already downloaded\n",
    "        already_uploaded = os.path.exists(new_name_with_path)\n",
    "        level += 1\n",
    "    if not check_folder_exists_g(new_name_with_path, folder_to_download_g) and (not already_uploaded or overwrite):\n",
    "        for item in list_blobs_g(folder_to_download_g):\n",
    "            file_path_g = item.name\n",
    "            file_name = trim_file_depth(file_path_g, start = file_depth)\n",
    "            local_path = os.path.join(new_name_with_path, file_name)\n",
    "            download_file_g(file_path_g, local_path)             \n",
    "\n",
    "\n",
    "def make_folder_g(folder_name, bkt = b):\n",
    "    blob = bkt.blob(folder_name + '/')\n",
    "    blob.upload_from_string('')\n",
    "\n",
    "def path_exists_g(file, bkt = b):\n",
    "    blob = bkt.blob(file)\n",
    "    return blob.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19c9e222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./Files_local already exists\n"
     ]
    }
   ],
   "source": [
    "if not path_exists_g('Files/'):\n",
    "    make_folder_g('Files')\n",
    "    make_folder_g('Files/Video_raw')\n",
    "    make_folder_g('Files/Video_ann')\n",
    "    make_folder_g('Files/Image_raw')\n",
    "    make_folder_g('Files/Image_ann')\n",
    "    make_folder_g('Files/Model')\n",
    "    make_folder_g('Files/Roboflow')\n",
    "make_folder(temp_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44a7c2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload_file_g(\"/mnt/linuxlab/home/mstaus1/Desktop/Shellfish_project_2024/Oyster Test Videos/GOPR0018.MP4\", \"GOPR1077-23.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "650cb013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Integrity Violated: Duplicate User with ab\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ab'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def add_user(name, password):\n",
    "    '''\n",
    "    Returns: name if query is successful, 0 if not\n",
    "    '''\n",
    "\n",
    "    try:\n",
    "        cur.execute(f\"INSERT INTO people (Username, Password, Time_Created) VALUES ('{name}', '{password}', CURRENT_TIMESTAMP );\")\n",
    "    except IntegrityError:\n",
    "        print(f\"Data Integrity Violated: Duplicate User with {name}\")\n",
    "    return name\n",
    "add_user(cur_name, '123')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b79cee66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./Files_local/problem_photo-38.jpg\n"
     ]
    }
   ],
   "source": [
    "def add_photo(name, fpath, notes = '', f_out = 'Files/Image_raw'):\n",
    "    '''\n",
    "    Returns: Index of added photo if successful, 0 if not\n",
    "    '''\n",
    "\n",
    "    im = Image.open(fpath)\n",
    "    width = im.size[0]\n",
    "    height = im.size[1]\n",
    "    fsize = os.stat(fpath).st_size\n",
    "    ext = get_ext(fpath)\n",
    "    f_temp = get_temp_fname(fpath)\n",
    "    im.save(f_temp)\n",
    "    cur.execute(f\"INSERT INTO raw_files (Username, Filepath, Local_Path, Size, Type, Extension, Notes, Width, Height, Time_Uploaded) VALUES ('{name}', 'REPLACE', '{f_temp}', {fsize}, 'Image', '{ext}', '{notes}', {width}, {height}, CURRENT_TIMESTAMP);\")\n",
    "    id = get_REPLACE_ID()\n",
    "\n",
    "    f_id_name_g = get_id_fname(f_out, fpath, id)\n",
    "    temp_fname = get_temp_fname(f_id_name_g)\n",
    "    print(temp_fname)\n",
    "#     print(f_temp, f_id_name)\n",
    "    upload_file_g(fpath, f_id_name_g)\n",
    "    cur.execute(f\"UPDATE raw_files SET Filepath = '{f_id_name_g}', Local_Path = '{temp_fname}' WHERE Raw_File_ID = {id};\")\n",
    "    return id\n",
    "\n",
    "SOME_IMAGE_PATH = \"/mnt/linuxlab/home/mstaus1/Desktop/Shellfish_project_2024/Jupyter_local_code/problem_photo.jpg\"\n",
    "\n",
    "id_raw_photo = add_photo(cur_name, fpath = SOME_IMAGE_PATH, notes = 'from HPC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a12356f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "def add_video(name, fpath, notes = '', f_out = 'Files/Video_raw'):\n",
    "    '''\n",
    "    Returns: Index of added video if successful, 0 if not\n",
    "    '''\n",
    "    cap = cv2.VideoCapture(fpath)\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fsize = os.stat(fpath).st_size\n",
    "    ext = get_ext(fpath)\n",
    "    \n",
    "    f_temp = get_temp_fname(fpath)\n",
    "    \n",
    "    cur.execute(f\"INSERT INTO raw_files (Username, Filepath, Local_Path, Size, Type, Extension, Notes, Width, Height, Time_Uploaded) VALUES ('{name}', 'REPLACE', 'REPLACE', {fsize}, 'Video', '{ext}', '{notes}', {width}, {height}, CURRENT_TIMESTAMP);\")\n",
    "    id = get_REPLACE_ID()\n",
    "    f_id_name_g = get_id_fname(f_out, fpath, id)\n",
    "    temp_path = get_temp_fname(f_id_name_g)\n",
    "#     print(temp_path)\n",
    "    \n",
    "#     print(fpath, f_id_name_g)\n",
    "    upload_file_g(fpath, f_id_name_g)\n",
    "    \n",
    "    sh.copyfile(fpath, f_id_name_g)\n",
    "\n",
    "    cur.execute(f\"UPDATE raw_files SET Filepath = '{f_id_name_g}', Local_Path = '{temp_path}' WHERE Raw_File_ID = {id};\")\n",
    "\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    color_order = 'RGB' # FIXME - cant figure out how to extract from cv2 object\n",
    "    \n",
    "    \n",
    "\n",
    "    cur.execute(f\"INSERT INTO videos (Raw_File_ID, FPS, Color_Order) VALUES ('{id}', '{fps}', '{color_order}');\")\n",
    "\n",
    "    return id\n",
    "\n",
    "SOME_VIDEO_PATH = \"/mnt/linuxlab/home/mstaus1/Desktop/Shellfish_project_2024/Jupyter_local_code/GOPR1077_tr.mp4\"\n",
    "id_raw_video = add_video(cur_name, SOME_VIDEO_PATH, notes='on new computer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "889a09ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_weight(path, ver): # one of ['n', 's', 'm', 'b', 'x', 'l']\n",
    "    valid_ver = ['n', 's', 'm', 'b', 'x', 'l']\n",
    "    if ver not in valid_ver:\n",
    "        assert InvalidVersionError('Invalid version selected. Must be one of: n, s, m, l, b, x,  for nano, small, medium, large, big extra big respectively')\n",
    "    prefix = \"https://github.com/THU-MIG/yolov10/releases/download/v1.1/yolov10\"\n",
    "    suffix = \".pt\"\n",
    "    web_path = prefix + ver + suffix\n",
    "    computer_path = os.path.join(path, 'yolov10' + ver + suffix)\n",
    "    if not os.path.exists(computer_path):\n",
    "        wget.download(web_path, out = path)\n",
    "    return computer_path\n",
    "\n",
    "# def get_weights(f_weights_g = 'Files/Weights'): # TODO - create weights table\n",
    "#     temp_weights = os.path.join(temp_folder, \"Weights\")\n",
    "#     make_folder(temp_weights)\n",
    "#     prefix = \"https://github.com/THU-MIG/yolov10/releases/download/v1.1/yolov10\"\n",
    "#     versions = ['n', 's', 'm', 'b', 'x', 'l']\n",
    "#     suffix = \".pt\"\n",
    "#     for ver in versions:\n",
    "#         cur_file = 'yolov10' + ver + suffix\n",
    "#         web_path = prefix + ver + suffix\n",
    "#         computer_path = os.path.join(temp_weights, cur_file)\n",
    "#         if not os.path.exists(computer_path):\n",
    "#             wget.download(web_path, out = temp_weights)\n",
    "    \n",
    "#     upload_folder_g(temp_weights, f_weights_g)\n",
    "    \n",
    "# get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0d3262bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_roboflow(api_key, workspace, project, version, download, location):\n",
    "    # if not os.path.exists(folder_roboflow):\n",
    "    rf = Roboflow(api_key = api_key)\n",
    "    project = rf.workspace(workspace).project(project)\n",
    "    version = project.version(version)\n",
    "    dat = version.download(download, location = location)\n",
    "    \n",
    "    fpath = os.path.join(location, 'data.yaml')\n",
    "    \n",
    "    with open(fpath) as f1:\n",
    "        lines = f1.readlines()\n",
    "    with open(fpath, 'w') as f2:\n",
    "        f2.writelines(lines[:-4])\n",
    "        f2.write(\"test: ../test/images\\ntrain: ../train/images\\nval: ../valid/images\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "56de2ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YGXCqFJKogQa7WbavueN\n",
      "oyster-pt-3\n",
      "oyt\n",
      "7\n",
      "yolov9\n",
      "YGXCqFJKogQa7WbavueN oyster-pt-3 oyt 7 yolov9 ./Files_local/oyster-pt-3_oyt_7_yolov9\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def add_roboflow(name, export_string, f_out = 'Files/Roboflow', f_weights = \"Files/Weights\", load = False):\n",
    "    \n",
    "    '''\n",
    "    Returns: Index of added roboflow if successful, 0 if not\n",
    "    '''\n",
    "    pattern_api = r'api_key\\s*=\\s*\"([^\"]+)\"'\n",
    "    api_key_lab = get_match(pattern_api, export_string)\n",
    "\n",
    "    pattern_workspace = r'rf\\.workspace\\(\"([^\"]+)\"\\)'\n",
    "    workspace_lab = get_match(pattern_workspace, export_string)\n",
    "\n",
    "    pattern_project = r'project\\(\"([^\"]+)\"\\)'\n",
    "    project_lab = get_match(pattern_project, export_string)\n",
    "\n",
    "    pattern_version = r'version\\((\\d+)\\)'\n",
    "    version_lab = get_match(pattern_version, export_string)\n",
    "\n",
    "    pattern_download = r'\\bdownload\\(\"([^\"]+)\"\\)'\n",
    "    download_lab = get_match(pattern_download, export_string)\n",
    "    \n",
    "#     f_temp = get_temp_fname(f_out)\n",
    "    folder_name = f'{workspace_lab}_{project_lab}_{version_lab}_{download_lab}'\n",
    "    f_temp = os.path.join(temp_folder, folder_name)\n",
    "    \n",
    "    folder_g = os.path.join(f_out, folder_name)\n",
    "#     folder_roboflow = f\"{f_out}/{workspace_lab}_{project_lab}_{version_lab}_{download_lab}\"\n",
    "    print(api_key_lab, workspace_lab, project_lab, version_lab, download_lab, f_temp)\n",
    "    if load:\n",
    "        download_roboflow(api_key_lab, workspace_lab, project_lab, version_lab, download_lab, f_temp)\n",
    "    \n",
    "    upload_folder_g(f_temp, folder_g)\n",
    "    \n",
    "    cur.execute(f\"INSERT INTO roboflow (Api_Key, Workspace, Project, Version, Download, Dataset_Location, Local_Path, Username, Timestamp) VALUES ('REPLACE', '{workspace_lab}', '{project_lab}', '{version_lab}', '{download_lab}', '{f_temp}', '{f_temp}', '{name}', CURRENT_TIMESTAMP);\")\n",
    "    \n",
    "    id = get_REPLACE_ID('Roboflow_ID', 'roboflow', 'Api_Key')\n",
    "    \n",
    "    cur.execute(f\"UPDATE roboflow SET Api_Key = '{api_key_lab}' WHERE Roboflow_ID = {id};\")\n",
    "    \n",
    "    return id\n",
    "    \n",
    "id_rob = add_roboflow(cur_name, \\\n",
    "\"\"\"\n",
    "rf = Roboflow(api_key=\"YGXCqFJKogQa7WbavueN\")\n",
    "project = rf.workspace(\"oyster-pt-3\").project(\"oyt\")\n",
    "version = project.version(7)\n",
    "dataset = version.download(\"yolov9\")\n",
    "\n",
    "\"\"\" )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "163a7f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# download_folder_g(\"Files/Roboflow/curvy-oysters_oysters-cjt7n_19_yolov8\", \"./Files_local/curvy-oysters_oysters-cjt7n_19_yolov8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "a43065a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload_file_g('./runs/detect/train2/weights/best.pt', f'./Files/Model/{2000}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "43f6f22a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Roboflow_ID': 21, 'Api_Key': 'YGXCqFJKogQa7WbavueN', 'Workspace': 'oyster-pt-3', 'Project': 'oyt', 'Version': 7, 'Download': 'yolov9', 'Username': 'ab', 'Timestamp': datetime.datetime(2024, 6, 28, 15, 27, 26), 'Dataset_Location': './Files_local/oyster-pt-3_oyt_7_yolov9', 'Local_Path': './Files_local/oyster-pt-3_oyt_7_yolov9'}\n",
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading Dataset Version Zip in ./Files_local/oyster-pt-3_oyt_7_yolov9 to yolov9:: 100%|██████████| 97782/97782 [00:07<00:00, 13885.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Dataset Version Zip to ./Files_local/oyster-pt-3_oyt_7_yolov9 in yolov9:: 100%|██████████| 4276/4276 [02:40<00:00, 26.68it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.2.45 available 😃 Update with 'pip install -U ultralytics'\n",
      "Ultralytics YOLOv8.1.34 🚀 Python-3.9.7 torch-2.3.1+cu121 CUDA:0 (NVIDIA GeForce RTX 4080, 16071MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=./Files_local/yolov10n.pt, data=/mnt/linuxlab/home/mstaus1/Desktop/Shellfish_project_2024/sql/Files_local/oyster-pt-3_oyt_7_yolov9/data.yaml, epochs=1, time=None, patience=100, batch=32, imgsz=640, save=True, save_period=-1, val_period=1, cache=False, device=None, workers=8, project=None, name=train2, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=False, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train2\n",
      "Overriding model.yaml nc=80 with nc=3\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1      9856  ultralytics.nn.modules.block.SCDown          [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1     36096  ultralytics.nn.modules.block.SCDown          [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1    249728  ultralytics.nn.modules.block.PSA             [256, 256]                    \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 17                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 20                  -1  1     18048  ultralytics.nn.modules.block.SCDown          [128, 128, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  1    282624  ultralytics.nn.modules.block.C2fCIB          [384, 256, 1, True, True]     \n",
      " 23        [16, 19, 22]  1    862498  ultralytics.nn.modules.head.v10Detect        [3, [64, 128, 256]]           \n",
      "YOLOv10n summary: 385 layers, 2708210 parameters, 2708194 gradients, 8.4 GFLOPs\n",
      "\n",
      "Transferred 493/595 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train2', view at http://localhost:6006/\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /mnt/linuxlab/home/mstaus1/Desktop/Shellfish_project_2024/sql/Fi\u001b[0m\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /mnt/linuxlab/home/mstaus1/Desktop/Shellfish_project_2024/sql/Files_local/oyster-pt-3_oyt_7_yolov9/train/labels.cache\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /mnt/linuxlab/home/mstaus1/Desktop/Shellfish_project_2024/sql/File\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /mnt/linuxlab/home/mstaus1/Desktop/Shellfish_project_2024/sql/Files_local/oyster-pt-3_oyt_7_yolov9/valid/images/Multiple_Moving_Oysters-_1717506724830_mp4-0006_jpg.rf.a691794bcc2ea0874810a7c6ae005bb1.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /mnt/linuxlab/home/mstaus1/Desktop/Shellfish_project_2024/sql/Files_local/oyster-pt-3_oyt_7_yolov9/valid/labels.cache\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001429, momentum=0.9) with parameter groups 95 weight(decay=0.0), 108 weight(decay=0.0005), 107 bias(decay=0.0)\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added ✅\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/train2\u001b[0m\n",
      "Starting training for 1 epochs...\n",
      "\n",
      "      Epoch    GPU_mem     box_om     cls_om     dfl_om     box_oo     cls_oo     dfl_oo  Instances       Size\n",
      "        1/1      7.08G      1.472      3.005      1.548      1.307      3.441   \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        565       5336    0.00607      0.225     0.0107    0.00517\n",
      "\n",
      "1 epochs completed in 0.008 hours.\n",
      "Optimizer stripped from runs/detect/train2/weights/last.pt, 5.8MB\n",
      "Optimizer stripped from runs/detect/train2/weights/best.pt, 5.8MB\n",
      "\n",
      "Validating runs/detect/train2/weights/best.pt...\n",
      "Ultralytics YOLOv8.1.34 🚀 Python-3.9.7 torch-2.3.1+cu121 CUDA:0 (NVIDIA GeForce RTX 4080, 16071MiB)\n",
      "YOLOv10n summary (fused): 285 layers, 2695586 parameters, 0 gradients, 8.2 GFLOPs\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        565       5336    0.00607      0.225     0.0107    0.00518\n",
      "         oyster-closed        565       3269     0.0109      0.289     0.0207    0.00982\n",
      "  oyster-indeterminate        565       1131    0.00549      0.373     0.0104    0.00529\n",
      "           oyster-open        565        936    0.00176     0.0118     0.0011   0.000453\n",
      "Speed: 0.1ms preprocess, 1.3ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
      "💡 Learn more at https://docs.ultralytics.com/modes/train\n"
     ]
    }
   ],
   "source": [
    "def add_model(roboflow_ID, f_out = \"Files/Model\", size_mod = 'n', weights_path_g = \"Files/Weights\", epochs = 10, batch = 32):\n",
    "    \n",
    "    # weights_path = download_weight(temp_folder, size_mod)\n",
    "    \n",
    "    \n",
    "    cur.execute(f\"SELECT * FROM roboflow WHERE Roboflow_ID = {roboflow_ID}\")\n",
    "    res = cur.fetchall()[-1]\n",
    "    print(res)\n",
    "\n",
    "    download_roboflow(res['Api_Key'], res['Workspace'], res['Project'], res['Version'], res['Download'], res['Dataset_Location'])\n",
    "    \n",
    "    \n",
    "#     dataset_loc_g = res['Dataset_Location']\n",
    "    \n",
    "#     dataset_temp = get_temp_fname(dataset_loc_g)\n",
    "    \n",
    "#     print(dataset_loc_g, dataset_temp)\n",
    "    \n",
    "#     download_folder_g(dataset_loc_g, dataset_temp)\n",
    "    \n",
    "    pt = os.path.join(os.getcwd(), res['Dataset_Location'][len('./'):], 'data.yaml')\n",
    "\n",
    "    !yolo task=detect mode=train epochs={epochs} batch={batch} plots=False model={weights_path} data={pt}\n",
    "    \n",
    "    samp_photo = os.path.join(res['Dataset_Location'], 'test', 'images')\n",
    "    first_photo = os.listdir(samp_photo)[0]\n",
    "    im = Image.open(os.path.join(samp_photo, first_photo))\n",
    "    width = im.size[0]\n",
    "    height = im.size[1]\n",
    "                                                               \n",
    "    cur.execute(f\"\"\"INSERT INTO models (Timestamp_Created, Model_Points_Path, Local_Path, Version, \n",
    "                 Hyperparams, Epoch, Batch, Model_Type, Width_Training_Images, Height_Training_Images, \n",
    "                 Size, Roboflow_ID) values (CURRENT_TIMESTAMP, 'REPLACE', 'REPLACE', 10, NULL, {epochs}, {batch}, \n",
    "                 'YOLO', {width}, {height}, '{size_mod}', {roboflow_ID})\n",
    "                 \"\"\")\n",
    "    \n",
    "    id_mod = get_REPLACE_ID(column_id = 'Model_ID', table='models', column_rep='Model_Points_Path')\n",
    "    pts_name = f'{id_mod}.pt'\n",
    "    model_path_g = os.path.join(f_out, pts_name)\n",
    "    pts_path = './runs/detect/train/weights/best.pt'\n",
    "    temp_save_path = os.path.join(temp_folder, pts_name)\n",
    "    upload_file_g(pts_path, model_path_g)\n",
    "    os.rename(pts_path, temp_save_path) # for later inference\n",
    "    \n",
    "    cur.execute(f\"UPDATE models SET Model_Points_Path = '{model_path_g}', Local_Path = '{temp_save_path}' WHERE Model_ID = {id_mod};\")\n",
    "\n",
    "    delete_folder('runs')\n",
    "    return id_mod\n",
    "\n",
    "id_mod = add_model(id_rob,epochs=1, size_mod = 'n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bd424187",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_photo_id = 0\n",
    "id_vid = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fc2e03b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Already Annotated\n"
     ]
    }
   ],
   "source": [
    "def ann_img(Raw_File_ID, Model_ID, threshold = 0.3, notes = '', f_out = 'Files/Image_ann'):\n",
    "    cur.execute(f\"SELECT * FROM raw_files WHERE Raw_File_ID = {Raw_File_ID}\")\n",
    "    cur_photo = cur.fetchall()[-1]\n",
    "\n",
    "    cur.execute(f\"SELECT * FROM models WHERE Model_ID = {Model_ID}\")\n",
    "    cur_model = cur.fetchall()[-1]\n",
    "\n",
    "    cur.execute(f\"SELECT * from annotated_files WHERE Model_ID = {Model_ID} AND Raw_File_ID = {Raw_File_ID}\")\n",
    "    res = cur.fetchall()\n",
    "    if res:\n",
    "        print('Image Already Annotated')\n",
    "        return res[-1]['Ann_File_ID']\n",
    "    \n",
    "    download_file_g(cur_photo['Filepath'], cur_photo['Local_Path'])\n",
    "    im = Image.open(cur_photo['Local_Path'])\n",
    "    np_img = np.array(im)\n",
    "\n",
    "    download_file_g(cur_model['Model_Points_Path'], cur_model['Local_Path'])\n",
    "    model = YOLOv10(cur_model['Local_Path'])\n",
    "    annot, num_oysters, tot_time, end_ann_data = annotate_image(np_img, model, conf_level = threshold)\n",
    "\n",
    "    cur.execute(f\"INSERT INTO annotated_files (Raw_File_ID, Model_ID, Annotated_Filepath, Time_to_Annotate, Confidence_Threshold, Notes, Timestamp) VALUES ('{Raw_File_ID}', '{Model_ID}', 'REPLACE', '{tot_time}', {threshold}, '{notes}', CURRENT_TIMESTAMP);\")\n",
    "    id = get_REPLACE_ID(column_id = 'Ann_File_ID', table='annotated_files', column_rep='Annotated_Filepath')\n",
    "    f_id_name_g = get_id_fname(f_out, cur_photo['Filepath'], id)\n",
    "    f_local = get_temp_fname(f_id_name_g)\n",
    "    print(f_local)\n",
    "\n",
    "    im_f = Image.fromarray(annot)\n",
    "    im_f.save(f_local)\n",
    "\n",
    "\n",
    "\n",
    "    upload_file_g(f_local, f_id_name_g)\n",
    "\n",
    "\n",
    "    cur.execute(f\"UPDATE annotated_files SET Annotated_Filepath = '{f_id_name_g}', Local_Path = '{f_local}' WHERE Ann_File_ID = {id};\")\n",
    "    cur.execute(f\"INSERT INTO annotated_photos (Ann_File_ID, Number_of_Oysters) VALUES ({id}, {num_oysters})\")\n",
    "\n",
    "    coord = end_ann_data.xyxy\n",
    "    conf = end_ann_data.confidence\n",
    "    names = end_ann_data.data['class_name']\n",
    "    class_num = end_ann_data.class_id\n",
    "    for idx in range(len(coord)):\n",
    "        coord_cur = coord[idx]\n",
    "        cur.execute(f\"INSERT INTO oysters_in_photo (Ann_File_ID, Confidence, X1, Y1, X2, Y2, Class, Class_Index) VALUES ({id}, {conf[idx]}, {coord_cur[0]}, {coord_cur[1]}, {coord_cur[2]}, {coord_cur[3]}, '{names[idx]}', {class_num[idx]});\")\n",
    "    return id\n",
    "#     except IntegrityError:\n",
    "#         print(\"An exception occurred\")\n",
    "#         print(IntegrityError)\n",
    "#         cur.execute(f\"SELECT Ann_File_ID FROM annotated_files WHERE Raw_File_ID = {Raw_File_ID} AND Model_ID = {Model_ID}\")\n",
    "#         return(cur.fetchall()[-1]['Ann_File_ID'])\n",
    "\n",
    "# cur.execute(f\"DELETE FROM oysters_in_photo WHERE Ann_File_ID = {ann_photo_id};\")  \n",
    "# cur.execute(f\"DELETE FROM annotated_photos WHERE Ann_File_ID = {ann_photo_id};\")\n",
    "# cur.execute(f\"DELETE FROM annotated_files WHERE Ann_File_ID = {ann_photo_id};\")\n",
    "\n",
    "ann_photo_id = ann_img(id_raw_photo, id_mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ff59cac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video already annotated\n"
     ]
    }
   ],
   "source": [
    "def ann_video(Raw_File_ID, Model_ID, notes = '', f_out = 'Files/Video_ann', threshold = 0.3):\n",
    "    cur.execute(f\"SELECT * FROM raw_files WHERE Raw_File_ID = {Raw_File_ID}\")\n",
    "    cur_video = cur.fetchall()[-1]\n",
    "\n",
    "    cur.execute(f\"SELECT * FROM models WHERE Model_ID = {Model_ID}\")\n",
    "    cur_model = cur.fetchall()[-1]\n",
    "\n",
    "    cur.execute(f\"SELECT * from annotated_files WHERE Model_ID = {Model_ID} AND Raw_File_ID = {Raw_File_ID}\")\n",
    "    res = cur.fetchall()\n",
    "    if res:\n",
    "        print('Video already annotated')\n",
    "        return res[-1]['Ann_File_ID']\n",
    "    \n",
    "    download_file_g(cur_model['Model_Points_Path'], cur_model['Local_Path'])\n",
    "    \n",
    "    model = YOLOv10(cur_model['Local_Path'])\n",
    "\n",
    "    download_file_g(cur_video['Filepath'], cur_video['Local_Path'])\n",
    "    \n",
    "    avg_oysters, time_s, out_path, ann_rate = annotate_video(cur_video['Local_Path'], model, out_location = temp_folder, conf_level = threshold)\n",
    "\n",
    "    cur.execute(f\"INSERT INTO annotated_files (Raw_File_ID, Model_ID, Annotated_Filepath, Time_to_Annotate, Confidence_Threshold, Notes, Timestamp) VALUES ('{Raw_File_ID}', '{Model_ID}', 'REPLACE', '{time_s * 1000}', {threshold}, '{notes}', CURRENT_TIMESTAMP);\")\n",
    "\n",
    "    id = get_REPLACE_ID(column_id = 'Ann_File_ID', table='annotated_files', column_rep='Annotated_Filepath')\n",
    "\n",
    "    f_id_name_g = get_id_fname(f_out, cur_video['Filepath'], id)\n",
    "    \n",
    "    f_local = get_temp_fname(f_id_name_g)\n",
    "#     print(f_local, out_path, f_id_name_g)\n",
    "    \n",
    "    cur.execute(f\"UPDATE annotated_files SET Annotated_Filepath = '{f_id_name_g}', Local_Path = '{f_local}' WHERE Ann_File_ID = {id};\")\n",
    "\n",
    "    os.rename(out_path, f_local)\n",
    "\n",
    "    cur.execute(f\"INSERT INTO annotated_videos (Ann_File_ID, Annotation_Rate, Tracing, Average_Number_of_Oysters) VALUES ({id}, {ann_rate}, 0, {avg_oysters})\")\n",
    "\n",
    "    upload_file_g(f_local, f_id_name_g)\n",
    "    \n",
    "    return id\n",
    "\n",
    "# cur.execute(f\"DELETE FROM oysters_in_photo WHERE Ann_File_ID = {id_vid};\")  \n",
    "# cur.execute(f\"DELETE FROM annotated_videos WHERE Ann_File_ID = {id_vid};\")\n",
    "# cur.execute(f\"DELETE FROM annotated_files WHERE Ann_File_ID = {id_vid};\")\n",
    "\n",
    "id_vid = ann_video(id_raw_video, id_mod)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
